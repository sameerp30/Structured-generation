{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from colbert.data import Queries\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert import Searcher\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from colbert import Indexer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Dec 09, 21:51:23] #> Note: Output directory /raid/nlp/sameer/ColBERT/experiments/NL2bash/indexes/nl2bash.nbits=2_pretrained_dummy already exists\n",
      "\n",
      "\n",
      "[Dec 09, 21:51:23] #> Will delete 10 files already at /raid/nlp/sameer/ColBERT/experiments/NL2bash/indexes/nl2bash.nbits=2_pretrained_dummy in 20 seconds...\n",
      "#> Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 1e-5,\n",
      "    \"maxsteps\": 400000,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 20000,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 64,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": null,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/raid\\/nlp\\/sameer\\/ColBERT\\/colbertv2.0\",\n",
      "    \"triples\": \"\\/future\\/u\\/okhattab\\/root\\/unit\\/experiments\\/2021.10\\/downstream.distillation.round2.2_score\\/round2.nway6.cosine.ib\\/examples.64.json\",\n",
      "    \"collection\": \"\\/raid\\/nlp\\/sameer\\/ColBERT\\/colbert_tsv_files\\/index_tldr_try.tsv\",\n",
      "    \"queries\": \"\\/future\\/u\\/okhattab\\/data\\/MSMARCO\\/queries.train.tsv\",\n",
      "    \"index_name\": \"nl2bash.nbits=2_pretrained_dummy\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/raid\\/nlp\\/sameer\\/ColBERT\\/experiments\",\n",
      "    \"experiment\": \"NL2bash\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2023-12\\/09\\/16.55.19\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Dec 09, 21:51:45] #> Loading collection...\n",
      "0M \n",
      "[Dec 09, 21:51:46] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Dec 09, 21:51:47] [0] \t\t # of sampled PIDs = 1582 \t sampled_pids[:3] = [853, 1500, 20]\n",
      "[Dec 09, 21:51:47] [0] \t\t #> Encoding 1582 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/nlp/sameer/anaconda3/envs/NL2bash/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]/raid/nlp/sameer/anaconda3/envs/NL2bash/lib/python3.10/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "  4%|▍         | 1/25 [00:04<01:55,  4.82s/it]/raid/nlp/sameer/anaconda3/envs/NL2bash/lib/python3.10/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "100%|██████████| 25/25 [02:15<00:00,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dec 09, 21:54:02] [0] \t\t avg_doclen_est = 119.88684844970703 \t len(local_sample) = 1,582\n",
      "[Dec 09, 21:54:03] [0] \t\t Creaing 4,096 partitions.\n",
      "[Dec 09, 21:54:03] [0] \t\t *Estimated* 189,660 embeddings.\n",
      "[Dec 09, 21:54:03] [0] \t\t #> Saving the indexing plan to /raid/nlp/sameer/ColBERT/experiments/NL2bash/indexes/nl2bash.nbits=2_pretrained_dummy/plan.json ..\n",
      "Clustering 180178 points in 128D to 4096 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.02 s\n",
      "  Iteration 19 (125.61 s, search 125.13 s): objective=39533.4 imbalance=1.485 nsplit=0       \n",
      "[0.034, 0.035, 0.033, 0.03, 0.031, 0.035, 0.033, 0.033, 0.033, 0.032, 0.032, 0.034, 0.033, 0.033, 0.033, 0.034, 0.031, 0.033, 0.032, 0.034, 0.033, 0.035, 0.032, 0.033, 0.032, 0.033, 0.036, 0.032, 0.034, 0.034, 0.032, 0.037, 0.035, 0.032, 0.033, 0.03, 0.035, 0.034, 0.032, 0.041, 0.035, 0.036, 0.035, 0.034, 0.033, 0.031, 0.032, 0.037, 0.036, 0.032, 0.032, 0.032, 0.035, 0.035, 0.032, 0.036, 0.038, 0.032, 0.037, 0.033, 0.034, 0.037, 0.033, 0.035, 0.035, 0.035, 0.034, 0.034, 0.031, 0.033, 0.036, 0.033, 0.031, 0.034, 0.033, 0.034, 0.034, 0.036, 0.033, 0.034, 0.034, 0.036, 0.034, 0.035, 0.031, 0.034, 0.033, 0.034, 0.032, 0.037, 0.032, 0.036, 0.032, 0.036, 0.034, 0.033, 0.037, 0.032, 0.036, 0.033, 0.032, 0.034, 0.033, 0.034, 0.034, 0.03, 0.034, 0.033, 0.034, 0.032, 0.034, 0.035, 0.034, 0.032, 0.034, 0.031, 0.035, 0.035, 0.033, 0.036, 0.031, 0.032, 0.032, 0.035, 0.032, 0.035, 0.033, 0.031]\n",
      "[Dec 09, 21:56:09] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500]) and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750])\n",
      "[Dec 09, 21:56:09] #> Got bucket_cutoffs = tensor([-2.5791e-02,  1.9672e-05,  2.5971e-02]) and bucket_weights = tensor([-0.0469, -0.0117,  0.0118,  0.0472])\n",
      "[Dec 09, 21:56:09] avg_residual = 0.03365685045719147\n",
      "[Dec 09, 21:56:09] [0] \t\t #> Encoding 1582 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▍         | 1/25 [00:05<02:02,  5.12s/it]\u001b[A\n",
      "  8%|▊         | 2/25 [00:10<01:58,  5.17s/it]\u001b[A\n",
      " 12%|█▏        | 3/25 [00:14<01:48,  4.93s/it]\u001b[A\n",
      " 16%|█▌        | 4/25 [00:20<01:46,  5.07s/it]\u001b[A\n",
      " 20%|██        | 5/25 [00:25<01:42,  5.13s/it]\u001b[A\n",
      " 24%|██▍       | 6/25 [00:30<01:34,  4.99s/it]\u001b[A\n",
      " 28%|██▊       | 7/25 [00:34<01:26,  4.82s/it]\u001b[A\n",
      " 32%|███▏      | 8/25 [00:39<01:21,  4.79s/it]\u001b[A\n",
      " 36%|███▌      | 9/25 [00:44<01:19,  4.94s/it]\u001b[A\n",
      " 40%|████      | 10/25 [00:49<01:12,  4.81s/it]\u001b[A\n",
      " 44%|████▍     | 11/25 [00:54<01:09,  5.00s/it]\u001b[A\n",
      " 48%|████▊     | 12/25 [00:59<01:05,  5.07s/it]\u001b[A\n",
      " 52%|█████▏    | 13/25 [01:05<01:03,  5.31s/it]\u001b[A\n",
      " 56%|█████▌    | 14/25 [01:10<00:57,  5.26s/it]\u001b[A\n",
      " 60%|██████    | 15/25 [01:16<00:54,  5.47s/it]\u001b[A\n",
      " 64%|██████▍   | 16/25 [01:21<00:47,  5.31s/it]\u001b[A\n",
      " 68%|██████▊   | 17/25 [01:27<00:43,  5.47s/it]\u001b[A\n",
      " 72%|███████▏  | 18/25 [01:33<00:38,  5.53s/it]\u001b[A\n",
      " 76%|███████▌  | 19/25 [01:38<00:33,  5.53s/it]\u001b[A\n",
      " 80%|████████  | 20/25 [01:44<00:28,  5.62s/it]\u001b[A\n",
      " 84%|████████▍ | 21/25 [01:49<00:21,  5.44s/it]\u001b[A\n",
      " 88%|████████▊ | 22/25 [01:55<00:16,  5.64s/it]\u001b[A\n",
      " 92%|█████████▏| 23/25 [02:01<00:11,  5.54s/it]\u001b[A\n",
      " 96%|█████████▌| 24/25 [02:06<00:05,  5.47s/it]\u001b[A\n",
      "100%|██████████| 25/25 [02:10<00:00,  5.20s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dec 09, 21:58:20] [0] \t\t #> Saving chunk 0: \t 1,582 passages and 189,661 embeddings. From #0 onward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [02:12, 132.37s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 50.73it/s]\n",
      "  0%|          | 0/4096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dec 09, 21:58:21] [0] \t\t #> Checking all files were saved...\n",
      "[Dec 09, 21:58:21] [0] \t\t Found all files!\n",
      "[Dec 09, 21:58:21] [0] \t\t #> Building IVF...\n",
      "[Dec 09, 21:58:21] [0] \t\t #> Loading codes...\n",
      "[Dec 09, 21:58:21] [0] \t\t Sorting codes...\n",
      "[Dec 09, 21:58:21] [0] \t\t Getting unique codes...\n",
      "[Dec 09, 21:58:21] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Dec 09, 21:58:21] #> Building the emb2pid mapping..\n",
      "[Dec 09, 21:58:21] len(emb2pid) = 189661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [00:00<00:00, 16177.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dec 09, 21:58:22] #> Saved optimized IVF to /raid/nlp/sameer/ColBERT/experiments/NL2bash/indexes/nl2bash.nbits=2_pretrained_dummy/ivf.pid.pt\n",
      "[Dec 09, 21:58:22] [0] \t\t #> Saving the indexing metadata to /raid/nlp/sameer/ColBERT/experiments/NL2bash/indexes/nl2bash.nbits=2_pretrained_dummy/metadata.json ..\n",
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(nranks=1, experiment=\"NL2bash\")):\n",
    "\n",
    "        config = ColBERTConfig(\n",
    "            nbits=2,\n",
    "            root=\"/raid/nlp/sameer/ColBERT/checkpoints\",\n",
    "        )\n",
    "        indexer = Indexer(checkpoint=\"/raid/nlp/sameer/ColBERT/colbertv2.0\", config=config)\n",
    "        indexer.index(name=\"nl2bash.nbits=2_pretrained_dummy\", collection=\"/raid/nlp/sameer/ColBERT/colbert_tsv_files/index_tldr_try.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dec 09, 22:00:48] #> Loading collection...\n",
      "0M \n",
      "[Dec 09, 22:00:49] #> Loading codec...\n",
      "[Dec 09, 22:00:49] #> Loading IVF...\n",
      "[Dec 09, 22:00:49] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1204.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dec 09, 22:00:49] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 45.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dec 09, 22:00:50] #> Loading the queries from /raid/nlp/sameer/ColBERT/colbert_tsv_files/queries.tsv ...\n",
      "[Dec 09, 22:00:50] #> Got 1081 queries. All QIDs are unique.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1081it [02:05,  8.64it/s]\n"
     ]
    }
   ],
   "source": [
    "ranks = None\n",
    "with Run().context(RunConfig(nranks=1, experiment=\"NL2bash\")):\n",
    "\n",
    "        config = ColBERTConfig(\n",
    "            root=\"/raid/nlp/sameer/ColBERT/checkpoints\",\n",
    "            # checkpoint=\"/raid/nlp/sameer/ColBERT/experiments/msmarco/none/2023-11/17/09.35.40/checkpoints/colbert\"\n",
    "        )\n",
    "        searcher = Searcher(index=\"nl2bash.nbits=2_pretrained_dummy\", config=config)\n",
    "        queries = Queries(\"/raid/nlp/sameer/ColBERT/colbert_tsv_files/queries.tsv\")\n",
    "        ranking = searcher.search_all(queries, k=10)\n",
    "        ranks = ranking.flat_ranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NL2bash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
